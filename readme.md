# âš–ï¸ Legal Chatbot â€“ Law Assistant Powered by LLMs

A conversational legal chatbot that uses semantic search and a local LLM (via Ollama) to provide easy-to-understand legal answers, with optional Creole translation. Powered by Flask, FAISS, SentenceTransformers, and Mistral.

---

## ğŸ“¦ Features

- âœ… Ask legal questions and get contextual answers  
- ğŸ“š Cites legal sources from a CSV knowledge base  
- ğŸ’¬ Supports English, Creole, or both  
- ğŸ” Semantic search using embeddings (MiniLM + FAISS)  
- ğŸ§  Local LLM inference via [Ollama](https://ollama.com/)  
- ğŸ³ Dockerized with `docker-compose`  

---

## ğŸ“ Project Structure

```
chatbot_app/
â”œâ”€â”€ app.py                # Flask app
â”œâ”€â”€ chatbot.py            # Embedding + Ollama API logic
â”œâ”€â”€ legal_qa.csv          # Local knowledge base
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ Dockerfile            # Flask app Docker setup
â”œâ”€â”€ docker-compose.yml    # Multi-service setup
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html         # Chat UI (Jinja2 + JS)
â””â”€â”€ static/
    â””â”€â”€ styles.css        # Optional CSS (or inline)
```

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/legal-chatbot.git
cd legal-chatbot
```

### 2. Run the App with Docker Compose

```bash
docker-compose up --build
```

This launches:
- `Flask` app on [http://localhost:5000](http://localhost:5000)
- `Ollama` model server on `localhost:11434`

---

### 3. Pull and Start the Model (Mistral)

Open another terminal and run:

```bash
docker exec -it legal-chatbot-ollama-1 ollama run mistral
```

Now the chatbot can generate responses using the Mistral LLM locally.

---

## ğŸ§  How It Works

1. The chatbot reads `legal_qa.csv` and generates vector embeddings using `sentence-transformers`.
2. Incoming user questions are compared using **FAISS** to find the closest legal topic.
3. A natural language prompt is constructed and passed to **Ollamaâ€™s REST API**.
4. The model (e.g., `mistral`) generates a response which is returned via Flask.
5. Optionally, responses are translated to **Guyanese Creole**.

---

## ğŸŒ Language Support

Choose your preferred language below the chat:
- ğŸ‡¬ğŸ‡§ English  
- ğŸï¸ Creole  
- ğŸŒ Both  

---

## ğŸ“œ Environment Variables

None required by default.

You can set these optionally in `docker-compose.yml`:

```yaml
environment:
  - FLASK_ENV=production
```

---

## âœ… Requirements (for manual setup)

If you're running locally without Docker:

```bash
pip install -r requirements.txt
```

---

## ğŸ§¹ Useful Commands

| Task                       | Command                                           |
|----------------------------|---------------------------------------------------|
| Build & start containers   | `docker-compose up --build`                      |
| Pull & run Mistral model   | `docker exec -it legal-chatbot-ollama-1 ollama run mistral` |
| Stop containers            | `docker-compose down`                            |

---

## ğŸ“„ License

MIT License Â© 2025

---

## ğŸ‘¨â€âš–ï¸ Maintainer

**Basudev Singh** â€“ *Supreme Court of Guyana* ğŸ‡¬ğŸ‡¾  
*Master of Science in Information Technology*
