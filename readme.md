# âš–ï¸ Legal Chatbot â€“ Law Assistant Powered by LLMs

A conversational legal chatbot that uses semantic search and a local LLM (via Ollama) to provide easy-to-understand legal answers, with optional Creole translation. Powered by Flask, FAISS, SentenceTransformers, and Mistral.

---

## ğŸ“¦ Features

- âœ… Ask legal questions and get contextual answers  
- ğŸ“š Cites legal sources from a CSV knowledge base  
- ğŸ’¬ Supports English, Creole, or both  
- ğŸ” Semantic search using embeddings (MiniLM + FAISS)  
- ğŸ§  Local LLM inference via [Ollama](https://ollama.com/)  
- ğŸ³ Dockerized with `docker-compose`  

---

## ğŸ“ Project Structure

```
chatbot_app/
â”œâ”€â”€ app.py                # Flask app
â”œâ”€â”€ chatbot.py            # Embedding + Ollama API logic
â”œâ”€â”€ legal_qa.csv          # Local knowledge base
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ Dockerfile            # Flask app Docker setup
â”œâ”€â”€ docker-compose.yml    # Multi-service setup
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html         # Chat UI (Jinja2 + JS)
â””â”€â”€ static/
    â””â”€â”€ styles.css        # Optional CSS (or inline)
```

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/legal-chatbot.git
cd legal-chatbot
```

### 2. Run the App with Docker Compose

```bash
docker-compose up --build
```

This launches:
- `Flask` app on [http://localhost:5000](http://localhost:5000)
- `Ollama` model server on `localhost:11434`

---

### 3. Pull and Start the Model (Mistral)

Open another terminal and run:

```bash
docker exec -it legal-chatbot-ollama-1 ollama run mistral
```

Now the chatbot can generate responses using the Mistral LLM locally.

---

## ğŸ§  How It Works

1. The chatbot reads `legal_qa.csv` and generates vector embeddings using `sentence-transformers`.
2. Incoming user questions are compared using **FAISS** to find the closest legal topic.
3. A natural language prompt is constructed and passed to **Ollamaâ€™s REST API**.
4. The model (e.g., `mistral`) generates a response which is returned via Flask.
5. Optionally, responses are translated to **Guyanese Creole**.

---

## ğŸŒ Language Support

Choose your preferred language below the chat:
- ğŸ‡¬ğŸ‡§ English  
- ğŸï¸ Creole  
- ğŸŒ Both  

---

## ğŸ“œ Environment Variables

None required by default.

You can set these optionally in `docker-compose.yml`:

```yaml
environment:
  - FLASK_ENV=production
```

---

## âœ… Requirements (for manual setup)

If you're running locally without Docker:

```bash
pip install -r requirements.txt
```

---

## ğŸ³ Docker & Development Setup

This project uses **Docker Compose** to manage two services:

- ğŸ§  `ollama` â€“ runs the Mistral model locally via REST API  
- ğŸ’¬ `app` â€“ your Flask-based legal chatbot  

---

### ğŸ“¦ Prerequisites

- Docker Desktop  
- Docker Compose (v2+)  
- Internet connection (first-time model pull only)  

---

### ğŸš€ Quick Start

```
docker-compose up --build
```

Then open: [http://localhost:5000](http://localhost:5000)

Ollama will auto-run the `mistral` model. The Flask app supports hot-reloading for fast dev cycles.

---

### ğŸ” Fast Iteration (No Rebuilds Needed)

Thanks to volume mounting:

- Code & template changes (e.g., `chatbot.py`, `chat.html`, `legal_qa.csv`) auto-reload
- No rebuild or restart needed in most cases

To quickly restart just the app:

```
docker-compose restart app
```

---

### ğŸ§  Model Persistence

- The Mistral model is cached in a Docker volume (`ollama_models`)
- It is pulled once, even if you restart or rebuild
- Ollama wonâ€™t re-download it every time

---

### âš™ï¸ Key Dev Config Highlights

```yaml
volumes:
  - .:/app                   # Live-reload Flask code
command:
  flask run --host=0.0.0.0 --port=5000
```

---

### ğŸ’¡ Common Commands

| Task                        | Command                          |
|-----------------------------|-----------------------------------|
| Start everything            | `docker-compose up --build`      |
| Restart only the app        | `docker-compose restart app`     |
| Stop all containers         | `docker-compose down`            |
| Rebuild just the Flask app  | `docker-compose build app`       |
| View logs                   | `docker-compose logs -f app`     |

---

For production: use a WSGI server like **Gunicorn** instead of the Flask dev server.

---

## ğŸ“„ License

MIT License Â© 2025

---

## ğŸ‘¨â€âš–ï¸ Maintainer

**Basudev Singh** â€“ *Supreme Court of Guyana* ğŸ‡¬ğŸ‡¾  
*Master of Science in Information Technology*
